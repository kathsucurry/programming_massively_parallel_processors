{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2781ae1f",
   "metadata": {},
   "source": [
    "# Notebook to generate test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72cd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import idx2numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23de89cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "LABEL_SIZE  = 10\n",
    "SAMPLE_SIZE = 3\n",
    "OUTPUT_PATH = \"outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60881f",
   "metadata": {},
   "source": [
    "## Load images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799ed69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the paths if needed.\n",
    "file_image_path = \"../../../Dataset/mnist/train-images-idx3-ubyte\"\n",
    "file_label_path = \"../../../Dataset/mnist/train-labels-idx1-ubyte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2abe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = idx2numpy.convert_from_file(file_image_path)\n",
    "all_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab52fbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x740cfa148920>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize an example.\n",
    "plt.imshow(all_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "392c364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = idx2numpy.convert_from_file(file_label_path)\n",
    "all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8f495f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ab700",
   "metadata": {},
   "source": [
    "In `run_tests.cu`, the batch happens to have the first (ordered) 3 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936ed669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 28, 28), (3,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = all_images[:SAMPLE_SIZE]\n",
    "labels = all_labels[:SAMPLE_SIZE]\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2252a50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the labels into one-hot encodings.\n",
    "one_hot_labels = np.zeros((SAMPLE_SIZE, LABEL_SIZE))\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    one_hot_labels[i, labels[i]] = 1\n",
    "one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dfbf4",
   "metadata": {},
   "source": [
    "## Preprocessing images\n",
    "\n",
    "In the test, we run two preprocessing steps on the images:\n",
    "\n",
    "1) Normalize to 0..1\n",
    "2) Add 2px padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76cb7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize images and convert to tensor.\n",
    "train_X = torch.tensor(images / 255, requires_grad=True)\n",
    "# Add padding and the input channel dimension.\n",
    "train_X = torch.nn.ZeroPad2d(2)(train_X).unsqueeze(1).to(torch.float32)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed14d82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = torch.tensor(one_hot_labels)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1329c",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1a44405",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_weights_file_path = \"inputs/weights_linear.txt\"\n",
    "conv2d_weights_file_path = \"inputs/weights_conv2d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c66f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tensor_from_file(file_path: str) -> np.ndarray:\n",
    "    file = open(file_path, \"r\")\n",
    "    str_content = file.read()\n",
    "    return torch.tensor([float(value) for value in str_content.split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d4992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_weights = read_tensor_from_file(linear_weights_file_path).reshape((LABEL_SIZE, 784))\n",
    "conv2d_weights = read_tensor_from_file(conv2d_weights_file_path).reshape((4, 1, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54935e45",
   "metadata": {},
   "source": [
    "## Run model for 1 iteration\n",
    "\n",
    "The forward pass consists of the following layers:\n",
    "\n",
    "- Convolution layer: 4 filters, each 5 x 5 --> output dim = 4 x 28 x 28\n",
    "- Sigmoid layer\n",
    "- Pooling layer: kernel size 2 x 2 --> output dim = 14 x 14\n",
    "- Flatten layer --> output dim = 784 (4 filters x 14 x 14)\n",
    "- Linear layer (784 x 10)\n",
    "- Softmax layer\n",
    "\n",
    "To compute loss, we use cross entropy loss on the *output of the linear layer*. Recall that cross entropy = log softmax + NLLLoss.\n",
    "\n",
    "There are four sets of outputs we need for testing the C/CUDA code:\n",
    "\n",
    "1) The output of each layer\n",
    "2) The result of running loss\n",
    "3) The gradients of each layer after running `.backward()`\n",
    "4) The updated weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea8bf2",
   "metadata": {},
   "source": [
    "Since the weights have been generated, we create custom classes for Conv2D and Linear layers to replace the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e08710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWeightConv2D(torch.nn.Conv2d):\n",
    "    def __init__(self, custom_weights):\n",
    "        super(CustomWeightConv2D, self).__init__(custom_weights.shape[0], custom_weights.shape[1], custom_weights.shape[2], bias=False)\n",
    "        self.weight = torch.nn.Parameter(custom_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "730506ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWeightLinear(torch.nn.Linear):\n",
    "    def __init__(self, custom_weights):\n",
    "         super(CustomWeightLinear, self).__init__(*custom_weights.shape, bias=False)\n",
    "         self.weight = torch.nn.Parameter(custom_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ab07b",
   "metadata": {},
   "source": [
    "### Get output of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7010a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    CustomWeightConv2D(conv2d_weights),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Flatten(),\n",
    "    CustomWeightLinear(linear_weights),\n",
    "    torch.nn.Softmax(dim=1),\n",
    "]\n",
    "layers_label = [\n",
    "    \"layer0_conv2d\",\n",
    "    \"layer1_sigmoid\",\n",
    "    \"layer2_maxpool\",\n",
    "    \"layer3_flatten\",\n",
    "    \"layer4_linear\",\n",
    "    \"layer5_softmax\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ac0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_output = []\n",
    "for layer_i, layer in enumerate(layers):\n",
    "    if layer_i == 0:\n",
    "        layers_output.append(layer(train_X))\n",
    "    else:\n",
    "        layers_output.append(layer(layers_output[layer_i - 1]))\n",
    "    layers_output[layer_i].retain_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd9f6b",
   "metadata": {},
   "source": [
    "### Get loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84e58fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9225, dtype=torch.float64, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss_output = loss(layers_output[4], train_y)\n",
    "loss_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15328c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(OUTPUT_PATH, f\"output_loss.txt\"), loss_output.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9058a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ef4d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_label, layer_output in zip(layers_label, layers_output):\n",
    "    np.savetxt(os.path.join(OUTPUT_PATH, f\"output_{layer_label}.txt\"), layer_output.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f25c5",
   "metadata": {},
   "source": [
    "### Get gradients of each layer output/weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a1a5e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0_conv2d\tgrad dim:  torch.Size([3, 4, 28, 28])\n",
      "layer1_sigmoid\tgrad dim:  torch.Size([3, 4, 28, 28])\n",
      "layer2_maxpool\tgrad dim:  torch.Size([3, 4, 14, 14])\n",
      "layer3_flatten\tgrad dim:  torch.Size([3, 784])\n",
      "layer4_linear\tgrad dim:  torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "for layer_label, layer_output in zip(layers_label[:-1], layers_output[:-1]):\n",
    "    print(f\"{layer_label}\\tgrad dim: \", layer_output.grad.shape)\n",
    "    np.savetxt(os.path.join(OUTPUT_PATH, f\"dy_{layer_label}.txt\"), layer_output.grad.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf919d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Store conv2d weight grad.\n",
    "print(layers[0].weight.grad.shape)\n",
    "np.savetxt(os.path.join(OUTPUT_PATH, f\"weight_grad_layer0_conv2d.txt\"), layers[0].weight.grad.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96578629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "# Store linear weight grad.\n",
    "print(layers[4].weight.grad.shape)\n",
    "np.savetxt(os.path.join(OUTPUT_PATH, f\"weight_grad_layer4_linear.txt\"), layers[4].weight.grad.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4276218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
